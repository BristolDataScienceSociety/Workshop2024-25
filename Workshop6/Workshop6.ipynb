{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "In this workshop, we will dive into the foundational concepts of linear regression, a cornerstone in machine learning. This includes understanding how models predict relationships between variables and minimize errors to improve accuracy. Along the way, you'll also get hands-on experience with Python libraries like scikit-learn for implementing these concepts. Whether you're new to the math or the code, this workshop provides a gradual and practical approach to mastering linear regression.\n",
    "\n",
    "Note: You can skip the mathematical derivations if you are not interested in knowing the underlying principles."
   ],
   "id": "2805c4285cdb0fa5"
  },
  {
   "cell_type": "markdown",
   "id": "b1e357738011c37f",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "**_Linear regression_** is a model that estimates the linear relationship between a scalar response (dependent variable) and one or more explanatory variables (regressor or independent variable).\n",
    "\n",
    "**_Loss function_** is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. It represents the \"distance\" between the predicted results and the real results.\n",
    "\n",
    "Minimising the loss function means the model is getting more accurate.\n",
    "\n",
    "We usually use **_Mean Square Error (MSE)_** as the loss function of linear models. It measures the average squared difference between the estimated values and the true value.\n",
    "\n",
    "The following graph shows the relationship between a single explanatory variable $ x $, a target variable $ y $, the regression line (blue line) and noises (green lines) for each instance.\n",
    "\n",
    "<img src=\"img/linear_explaination.png\" alt=\"Graph explaining Linear Regression\" height=400px>\n",
    "\n",
    "_Guidence for understanding equations:_\n",
    "\n",
    "1. $ i $ is related to $ i $-th instance in the dataset, and $ N $ is the number of instances in the dataset.\n",
    "2. $ k $ is related to $ k $-th feature in the dataset, and $ K $ is the number of features in the dataset (excluding the target feature $ y $).\n",
    "3. $ y $ stands for target vector, it has length of $ N $.\n",
    "4. $ X $ stands for regressor matrix, it has size of $ K * N $.\n",
    "5. $ β $ stands for coefficient vector, it has length of $ K $.\n",
    "6. $ ε $ stands for error variable (which takes value for each target in $ y $), it has length of $ N $.\n",
    "7. $ m $ is related to $ m $-th class in logistic regression, and $ M $ is the number of classes in the regression.\n",
    "8. Some eqautions have tag numbers at the right hand side, they are facts or results of derivation. Other eqautions without a tag number are steps of derivation.\n",
    "9. Some equations begin with $ \\text{for all } \\ldots = \\ldots: $, this kind of clause takes effects to the last occurance of the variable or until the definition of variable is replaced by another clause (e.g. summation notation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52caf075748982",
   "metadata": {},
   "source": [
    "## 1. Brief intro to scikit-learn\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/index.html) is used for machine learning in Python\n",
    "- Simple and efficient tools for predictive data analysis\n",
    "- Accessible to everybody, and reusable in various contexts\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "- Open source, commercially usable - BSD license\n",
    "\n",
    "Installation of scikit-learn\n",
    "- using either pip:\n",
    "  ```sh\n",
    "  python -m venv sklearn-env\n",
    "  sklearn-env\\Scripts\\activate  # activate\n",
    "  pip install -U scikit-learn\n",
    "  ```\n",
    "- or conda if you're using anaconda distribution:\n",
    "  ```sh\n",
    "  conda create -n sklearn-env -c conda-forge scikit-learn\n",
    "  conda activate sklearn-env\n",
    "  ```\n",
    "\n",
    "Then import modules necessary for machine learning in Python:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b4351700",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "afc49d7f7dc59d4",
   "metadata": {},
   "source": [
    "## 2. Simple linear regression\n",
    "\n",
    "A model with exactly one **_explanatory (independent) variable_** is a simple linear regression.\n",
    "\n",
    "This relationship is modeled through a **_disturbance term_** or **_error variable_** $ε$, an unobserved random variable that adds \"noise\" to the linear relationship between the dependent variable $ y $ and regressor variable $ x $. Given a dataset $ \\lbrace x_i, y_i \\rbrace ^N_{i=1} $, the model takes the form\n",
    "\n",
    "$$\n",
    "    \\text{for all } i = 1, \\ldots, N: \\qquad y_i = β_1x_i + β_0 + ε_i \\tag{1}\n",
    "$$\n",
    "\n",
    "A stardard process of doing linear regression:\n",
    "1. Load dataset (usually use **pandas**, review workshop 4)\n",
    "2. Split into regressor matrix $ X $ and target vector $ y $ (usually use **numpy**, review workshop 3)\n",
    "3. Split into train and test datasets\n",
    "4. Data preprocessing\n",
    "5. Instantiate linear regression model\n",
    "6. Fit train dataset into model\n",
    "7. Test accuracy of model with test dataset\n",
    "8. Data and result visualisation (usually **matplotlib.pyplot**, review workshop 5)\n",
    "\n",
    "_In the first few examples, not all steps are accomplished. In each example we will implement more steps to provide a better learning experience._"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# use synthetic data x-y from data/simple_linear.csv to do a simple linear regression\n",
    "\n",
    "\"\"\"\n",
    "Synthetic dataset information:\n",
    "Features: Math and Physics scores, and Physics score is the target.\n",
    "Number of instances: 95\n",
    "It is generated by the equation y = X + β0 + ε, where\n",
    "    y is Physics score,\n",
    "    X is Math score, comes from discrete uniform distribution U(0, 100),\n",
    "    β0 = -5 (the y-intercept),\n",
    "    ε comes from normal distribution N(0, 7.5**2).\n",
    "\"\"\"\n",
    "\n",
    "# step 1: read csv file \"data/simple_linear.csv\" and store as numpy array (np.array)\n",
    "dataset = pd.read_csv(\"data/simple_linear.csv\").to_numpy()\n",
    "\n",
    "# step 2: split into X and y using np.split\n",
    "# TODO: write your code here\n",
    "\n",
    "# step 5: instantiate linear regression using sklearn\n",
    "# TODO: write your code here\n",
    "\n",
    "# step 6: fit the data\n",
    "# TODO: write your code here\n",
    "\n",
    "# step 8: visualisation by using matplotlib (review workshop 5 if you forget how to do it)\n",
    "# TODO: write your code here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0a15a2e2b7dd995",
   "metadata": {},
   "source": [
    "_Think: what do you notice from the plot?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9278dc1033a8a",
   "metadata": {},
   "source": [
    "## 3. Multiple linear regression\n",
    "\n",
    "A model with two or more explanatory (independent) variables is a multiple linear regression.\n",
    "\n",
    "#### **Mathematical definition**\n",
    "\n",
    "Given a dataset $ \\lbrace x_{i1}, \\ldots, x_{ik}, y_i \\rbrace ^N_{i=1} $, the multiple linear model takes the form\n",
    "\n",
    "$$\n",
    "    y = Xβ + ε \\tag{2}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\qquad\n",
    "    X = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}\n",
    "      = \\begin{bmatrix}\n",
    "            1 & x_{11} & \\cdots & x_{1k} \\\\\n",
    "            1 & x_{21} & \\cdots & x_{2k} \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            1 & x_{n1} & \\cdots & x_{nk}\n",
    "        \\end{bmatrix}, \\qquad\n",
    "    β = \\begin{bmatrix} β_0 \\\\ β_1 \\\\ \\vdots \\\\ β_k \\end{bmatrix}, \\qquad\n",
    "    ε = \\begin{bmatrix} ε_1 \\\\ ε_2 \\\\ \\vdots \\\\ ε_n \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "To derive this equation we start from simple linear regression:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\text{for all } i = 1, \\ldots, N: \\qquad\n",
    "            y_i &= β_0 + β_1x_{i1} + ε_i & \\text{equation (1)}\\\\\n",
    "            y_i &= β_0 + β_1x_{i1} + β_2x_{i2} + ε_i & \\text{add feature }x_{i2} \\\\\n",
    "            y_i &= β_0 + β_1x_{i1} + \\cdots + β_kx_{ik} + ε_i & \\text{add to K features} \\\\\n",
    "            y_i &= β_0x_{i0} + β_1x_{i1} + \\cdots + β_kx_{ik} + ε_i & x_{i0} = 1 \\\\\n",
    "            y_i &= X_iβ + ε_i & \\text{fold coefficients and features}\\\\\n",
    "            y &= Xβ + ε & \\text{fold instances}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "When you understand the equation above, you can understand that linear regression is just a transformation from a regressor matrix $ X $ of shape $ K * N $, to a target vector $ y $ of length $ N $, by evaluating the dot product $ X \\cdot β $. The training of linear regression is to find a $ β $ such that loss function (here is mean square error) is minimised.\n",
    "\n",
    "#### **Parameter estimation**\n",
    "\n",
    "The best coefficient vector $ β $ can be easily found by derivative of loss function.\n",
    "\n",
    "The definition of mean sqaure error (MSE) is the mean of the squares of the errors, as follows:\n",
    "\n",
    "$$\n",
    "    MSE(\\hat{y}) = \\frac 1N \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\tag{3}\n",
    "$$\n",
    "\n",
    "where $ y $ is observed target values (read from dataset) and $ \\hat{y} $ is predicted target values (predicted by regression model).\n",
    "\n",
    "In linear regression, there is exactly one minimum point of loss function, we can find that point by letting derivative to be zero.\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        MSE(\\hat{y}) &= \\frac 1N \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 & \\text{equation (3)} \\\\\n",
    "        MSE(Xβ) &= \\frac 1N \\sum_{i=1}^N (y_i - X_iβ)^2 & \\hat{y} \\text{ is equivalent to } Xβ \\\\\n",
    "        MSE(Xβ) &= \\frac 1N (y - Xβ)^T(y - Xβ) & \\text{in matrix notation} \\\\\n",
    "        MSE(Xβ) &= \\frac 1N (y^T y - y^T Xβ - (Xβ)^T y + (Xβ)^T Xβ) & \\text{distributive law} \\\\\n",
    "        MSE(Xβ) &= \\frac 1N (y^T y - 2 y^T Xβ + β^T X^T Xβ) & y^T Xβ = (Xβ)^T y \\\\\n",
    "        0 = \\frac{∂}{∂β} MSE(Xβ) &= \\frac 1N (- 2 y^T X + 2 β^T X^T X) & \\text{partial derivative over } β \\\\\n",
    "        y^T X &= β^T X^T X & \\text{rearrange}\\\\\n",
    "        β^T &= (X^T X)^{-1} y^T X & \\text{rearrange}\\\\\n",
    "        β &= X^T y (X^T X)^{-1} & \\text{transpose} \\tag{4}\n",
    "    \\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Assumptions\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable is linear.\n",
    "2. **Independence**: Observations are independent of each other.\n",
    "3. **Homoscedasticity**: The variance of the residuals (errors) is constant across all levels of the independent variables.\n",
    "4. **Normality of Residuals**: The residuals are normally distributed.\n",
    "5. **No Multicollinearity**: Independent variables are not highly correlated with each other (to ensure $X^TX$ has an inverse).\n",
    "6. **No Autocorrelation**: Residuals are not correlated with one another (important in time series data).\n",
    "\n",
    "#### Advantages\n",
    "1. **Simplicity**: Easy to understand, implement, and interpret.\n",
    "2. **Efficiency**: Computationally efficient, especially for small datasets.\n",
    "3. **Transparency**: Coefficients provide clear insights into the relationship between variables.\n",
    "4. **Baseline Model**: Useful as a baseline for comparison with more complex models.\n",
    "5. **Assumption-Based**: If assumptions are met, the model provides unbiased and optimal estimators.\n",
    "\n",
    "#### Disadvantages\n",
    "1. **Assumption-Dependent**: Violation of assumptions (e.g., non-linearity, multicollinearity, or heteroscedasticity) can lead to unreliable results.\n",
    "2. **Limited Flexibility**: Poor performance in capturing non-linear relationships unless polynomial or interaction terms are added.\n",
    "3. **Outlier Sensitivity**: Strongly influenced by outliers, which can skew results.\n",
    "4. **Feature Dependence**: Requires careful feature selection and scaling to handle multicollinearity and different data scales.\n",
    "5. **Overfitting Risk**: When adding too many predictors relative to the number of observations, overfitting may occur.\n",
    "6. **Low Predictive Power**: May not perform well with large datasets or complex relationships compared to advanced models like decision trees or neural networks.\n"
   ],
   "id": "d4076e21895f34c7"
  },
  {
   "cell_type": "code",
   "id": "ee98d63e9e56444f",
   "metadata": {},
   "source": [
    "# use synthetic data x-y from data/multiple_linear.csv to do a linear regression\n",
    "\n",
    "\"\"\"\n",
    "Synthetic dataset information:\n",
    "Features: 11 meaningless features and the last feature is the target\n",
    "Number of instances: 100\n",
    "It is generated by sklearn.datasets.make_sparse_uncorrelated(), the relationship is as follows:\n",
    "    X ~ N(0, 1)\n",
    "    y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]\n",
    "Only the first 4 features are informative. The remaining features are useless.\n",
    "\"\"\"\n",
    "\n",
    "# read csv file \"data/multiple_linear.csv\" and store as numpy array (np.array)\n",
    "# TODO: write your code here\n",
    "\n",
    "# split into X and y\n",
    "# TODO: write your code here\n",
    "\n",
    "# split into 80% train and 20% test datasets\n",
    "# TODO: write your code here\n",
    "\n",
    "# instantiate model and fit\n",
    "# TODO: write your code here\n",
    "\n",
    "# test accuracy (not constant due to train_test_split)\n",
    "# TODO: complete the code here\n",
    "print(f\"accuracy: {}\") # around 90%\n",
    "print(f\"effective coefficients: {}\") # approximately [1 2 -2 -1.5]\n",
    "print(f\"useless coefficients: {}\") # all approximately 0\n",
    "print(f\"y-intercept: {}\") # approximately 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd206c5d95734b2d",
   "metadata": {},
   "source": [
    "## 4. Polynomial regression\n",
    "\n",
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable $ x $ and the dependent variable $ y $ is modeled as an $ k $-th degree polynomial in $ x $.\n",
    "\n",
    "Given a dataset $ \\lbrace x_i, y_i \\rbrace ^N_{i=1} $, the polynomial regression model takes the form\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\text{for all} \\, i = 1, \\ldots, N: \\qquad\n",
    "            y_i &= β_0 + β_1x_i + β_2x^2_i + \\cdots + β_kx^k_i + ε_i \\tag{5} \\\\\n",
    "        y &= Xβ + ε\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\qquad\n",
    "    X = \\begin{bmatrix}\n",
    "            1 & x_1 & x^2_1 & \\cdots & x^k_1 \\\\\n",
    "            1 & x_2 & x^2_2 & \\cdots & x^k_2 \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            1 & x_n & x^2_n & \\cdots & x^k_n\n",
    "        \\end{bmatrix}, \\qquad\n",
    "    β = \\begin{bmatrix} β_0 \\\\ β_1 \\\\ \\vdots \\\\ β_k \\end{bmatrix}, \\qquad\n",
    "    ε = \\begin{bmatrix} ε_1 \\\\ ε_2 \\\\ \\vdots \\\\ ε_n \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "Note that the definition of the regressor matrix $ X $ is different from that of multiple linear regression.\n",
    "\n",
    "Polynomial regression is a special form of multiple linear regression. For least squares analysis, the computational and inferential problems of polynomial regression can be completely addressed using the techniques of multiple regression. This is done by treating $ x, x^2, \\ldots, x^k $ as being distinct independent variables in a multiple regression model.\n",
    "\n",
    "But how can we know what value of $ k $ to choose, we don't know how target $ y $ varies with feature $ x $.\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "id": "cb2dcc8882ec2ea9",
   "metadata": {},
   "source": [
    "# use synthetic data x-y from data/polynomial.csv to do a linear regression\n",
    "\n",
    "\"\"\"\n",
    "Synthetic dataset information:\n",
    "Features: 2 features x and y, y is the target\n",
    "Number of instances: 100\n",
    "It is generated by the following relationship:\n",
    "    x ~ U(-5, 5)\n",
    "    y(X) = 4 + 3 * x - 2 * x**2 + 1 * x**3 + ε\n",
    "    ε ~ N(0, 5**2)\n",
    "\"\"\"\n",
    "\n",
    "# read csv file from \"data/polynomial.csv\" and split as above\n",
    "dataset = pd.read_csv(\"data/polynomial.csv\").to_numpy()\n",
    "X, y = np.split(dataset, [1], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# make pipeline (just like composite function) and train\n",
    "model = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "pipe = make_pipeline(sklearn.preprocessing.PolynomialFeatures(degree=3), model) # PolynomialFeatures(degree=3) expand\n",
    "                                                                                # [x] to [1, x, x**2, x**3]\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# test accuracy (not constant due to train_test_split)\n",
    "print(f\"accuracy: {pipe.score(X_test, y_test)}\") # around 98%\n",
    "print(f\"coefficients: {model.coef_[0]}\") # approximately [4. 3. 2. 1.], the first coefficient is β0 (y-intercept)\n",
    "\n",
    "# draw data points and best fit curve by sampling\n",
    "plt.scatter(X, y, s=15, c=\"red\")\n",
    "x_samples = np.linspace(-5, 5)\n",
    "plt.plot(x_samples, pipe.predict(x_samples.reshape((50, 1))))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35205f41",
   "metadata": {},
   "source": [
    "98% accuracy is perfect, as you know the degree $ k $. What happens if you choose inproper degree $ k $?\n",
    "\n",
    "**_Underfitting_** occurs when a mathematical model cannot adequately capture the underlying structure of the data.\n",
    "\n",
    "An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Underfitting would occur, for example, when fitting a linear model to nonlinear data. Such a model will tend to have poor predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c1db650",
   "metadata": {},
   "source": [
    "# degree = 2\n",
    "# TODO: write your code here\n",
    "\n",
    "# test accuracy\n",
    "# TODO: write your code here\n",
    "\n",
    "# draw data points and best fit curve by sampling\n",
    "# TODO: write your code here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "715de1ec",
   "metadata": {},
   "source": [
    "**_Overfitting_** is the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably.\n",
    "\n",
    "An overfitted model is a mathematical model that contains more parameters than can be justified by the data. In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure."
   ]
  },
  {
   "cell_type": "code",
   "id": "879c7f27",
   "metadata": {},
   "source": [
    "# degree = 20\n",
    "# TODO: write your code here\n",
    "\n",
    "# test accuracy (not constant due to train_test_split)\n",
    "# TODO: write your code here\n",
    "\n",
    "# draw data points and best fit curve by sampling\n",
    "# TODO: write your code here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c318dc66",
   "metadata": {},
   "source": [
    "We can determine a proper degree $ k $ by experiments:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5fa43bcc",
   "metadata": {},
   "source": [
    "# test all degree 1 <= k <= 30\n",
    "results = [make_pipeline(sklearn.preprocessing.PolynomialFeatures(degree=k), model)\n",
    "           .fit(X_train, y_train).score(X_test, y_test) for k in range(1, 31)]\n",
    "\n",
    "# output best choice (usually k = 3)\n",
    "print(f\"best: degree {np.array(results).argmax() + 1}, accuracy {max(results)}\")\n",
    "\n",
    "# draw data points and best fit curve by sampling\n",
    "plt.plot(range(1, 31), results)\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.xlabel(\"degree k\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d127dd54c493754",
   "metadata": {},
   "source": [
    "## 5. Exercise: a real task\n",
    "\n",
    "**_Data preprocessing_** can refer to manipulation, filtration or augmentation of data before it is analyzed.\n",
    "\n",
    "The preprocessing pipeline used can often have large effects on the conclusions drawn from the downstream analysis. Thus, representation and quality of data is necessary before running any analysis.\n",
    "\n",
    "- **_Data cleaning_**: remove outliars (mean square error has the disadvantage of heavily weighting outliers), fix missing values, remove duplicated data, ...\n",
    "\n",
    "- **_Normalisation_**: rescale each feature such that it has a standard deviation of 1 and a mean of 0 (to ease the convergence or to create a completely different model fit compared to the fit with unscaled data).\n",
    "\n",
    "Use a real dataset for prediction, normalise dataset by `StandardScaler` and then randomly divide dataset into 80% train and 20% test. Test accuracy on the test set to see what our linear regression can do."
   ]
  },
  {
   "cell_type": "code",
   "id": "8353c69be0c21730",
   "metadata": {},
   "source": [
    "# use OpenML dataset \"vinnie\"\n",
    "\n",
    "\"\"\"\n",
    "Online dataset information:\n",
    "Features: Year, Field Goal Attempts, Field Goal (target)\n",
    "Number of instances: 380\n",
    "https://www.openml.org/search?type=data&status=active&id=519\n",
    "\"\"\"\n",
    "\n",
    "# download dataset\n",
    "dataset = sklearn.datasets.fetch_openml(\"vinnie\", version=1, as_frame=True)\n",
    "\n",
    "# split dataset to 80% train and 20% test\n",
    "# TODO: write your code here\n",
    "\n",
    "# instantiate normaliser and linear regression model\n",
    "# TODO: write your code here\n",
    "\n",
    "# fit train data\n",
    "# TODO: write your code here\n",
    "\n",
    "# test accuracy (around 80%)\n",
    "# TODO: write your code here\n",
    "\n",
    "# pairplot\n",
    "dataframe = pd.DataFrame(np.column_stack((dataset.data, dataset.target)),\n",
    "                         columns=[\"Year\", \"Field Goal Attempts\", \"Field Goal\"])\n",
    "sns.pairplot(dataframe)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14229e0a584501f",
   "metadata": {},
   "source": [
    "_Think: do you think the accuracy of this task is high or low, why?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0e332",
   "metadata": {},
   "source": [
    "## 6. Linear Separability\n",
    "\n",
    "In Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side.\n",
    "\n",
    "<img src=\"img/linearly_separable_red-blue_cropped.png\" alt=\"Linear Separability\" height=400px>\n",
    "\n",
    "This idea immediately generalizes to higher-dimensional Euclidean spaces if the line is replaced by a hyperplane.\n",
    "\n",
    "Mathematical definition: let $ X_0 $ and $ X_1 $ be two sets of points in an $ n $-dimensional Euclidean space. Then $ X_0 $ and $ X_1 $ are linearly separable if there exist $ z + 1 $ real numbers $ w_1, w_2, \\ldots, w_z, c $, such that\n",
    "\n",
    "$$\n",
    "    \\text{for all } x \\in X_0: \\sum_{k=1}^z w_kx_k > c \\text{ and for all } x \\in X_1: \\sum_{k=1}^z w_kx_k < c \\tag{6}\n",
    "$$\n",
    "\n",
    "where $ x_k $ is the $ k $-th component of $ x $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f21d07",
   "metadata": {},
   "source": [
    "## 7. Logistic regression (actually is used for classification)\n",
    "\n",
    "Logistic regression estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations).\n",
    "\n",
    "The logistic model transforms regressor matrix into probabilities of being classfied as each class.\n",
    "\n",
    "#### **Mathematical definition**\n",
    "\n",
    "We shall start with binomial logistic regression (classification with two classes). The logistic function is a **_sigmoid function_**, which takes any real input $ t $ and outputs a value between zero and one. The stardard logistic function $ σ: {\\Bbb R} \\to (0, 1) $ is defined as follows:\n",
    "\n",
    "$$\n",
    "    σ(t) = \\frac{e^t}{e^t + 1} = \\frac{1}{1 + e^{-t}} \\tag{7}\n",
    "$$\n",
    "\n",
    "A graph of the logistic function on the $ t $-interval $ (−6, 6) $ is shown below:\n",
    "\n",
    "![Logistic curver](img/logistic-curve.png)\n",
    "\n",
    "Let's assume that $ t $ is a linear function of a single explanatory variable $ x $:\n",
    "\n",
    "$$\n",
    "    t = β_0 + β_1x \\tag{8}\n",
    "$$\n",
    "\n",
    "So the general logistic function $ p: {\\Bbb R} \\to (0, 1) $ can be rewritten as\n",
    "\n",
    "$$\n",
    "    p(x) = σ(t) = \\frac{1}{1 + e^{-(β_0 + β_1x)}} \\tag{9}\n",
    "$$\n",
    "\n",
    "$ p(x) $ is interpreted as the probability of the dependent variable $ y $ equaling a success/case rather than a failure/non-case.\n",
    "\n",
    "The inverse of the logistic function $ σ $:\n",
    "\n",
    "$$\n",
    "    σ^{-1}(p) = {\\rm ln} \\left( \\frac{p}{1 - p} \\right) = β_0 + β_1x \\tag{10}\n",
    "$$\n",
    "\n",
    "Exponentiate both side:\n",
    "\n",
    "$$\n",
    "    \\frac{p}{1 - p} = e^{β_0 + β_1x} \\tag{11}\n",
    "$$\n",
    "\n",
    "This equation shows how the logistic model links logistic function to linear regression expression.\n",
    "\n",
    "Extending equations $ (8) $ to $ (11) $ so that $ t $ is a linear function of multiple explanatory variables $ x_1, x_2, \\ldots, x_k $:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        t &= β_0 + β_1x_1 + \\cdots + β_kx_k \\tag{12} \\\\\n",
    "        p = σ(t) &= \\frac{1}{1 + e^{-(β_0 + β_1x_1 + \\cdots + β_kx_k)}} \\tag{13} \\\\\n",
    "        σ^{-1}(p) = {\\rm ln} \\left( \\frac{p}{1 - p} \\right) &=\n",
    "            β_0 + β_1x_1 + \\cdots + β_kx_k \\tag{14}\\\\\n",
    "        \\frac{p}{1 - p} &= e^{β_0 + β_1x_1 + \\cdots + β_kx_k} \\tag{15}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "What about multinomial logistic regression (classification with multiple classes)?\n",
    "\n",
    "In the following paragraphs, we only consider a single instance in the dataset so variable $ i $ and number of instances $ n $ are omitted, $ y $ becomes a single target value and\n",
    "\n",
    "$$\n",
    "    X = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_k \\end{bmatrix}, \\qquad\n",
    "    β = \\begin{bmatrix} β_1 & β_2 & \\cdots & β_m \\end{bmatrix} =\n",
    "        \\begin{bmatrix}\n",
    "            β_{11} & β_{21} & \\cdots & β_{m1} \\\\\n",
    "            β_{12} & β_{22} & \\cdots & β_{m2} \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            β_{1k} & β_{2k} & \\cdots & β_{mk}\n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It can be seen as a set of independent binomial logistic regressions. In the case that we have $ K $ explanatory variables and $ M $ classes, we can choose outcome $ M $ (the last outcome) as the pivot, other $ M − 1 $ outcomes are separately regressed against the pivot outcome (each pair has a set of coefficients $ β_m $).\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\text{for all } m = 1, \\ldots, M - 1: \\qquad\n",
    "            \\frac{\\Pr(y = m)}{\\Pr(y = M)} &= e^{β_{m0} + β_{m1}x_1 + \\cdots + β_{mk}x_k} & \\text{equation (15)} \\\\\n",
    "        \\frac{\\Pr(y = m)}{\\Pr(y = M)} &= e^{Xβ_m} & \\text{fold} \\\\\n",
    "        \\Pr(y = m) &= \\Pr(y = M)e^{Xβ_m} & \\text{rearrange} \\tag{16}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "where $ \\Pr $ stands for **_probability mass function_** and $ \\Pr(y = m) $ is the probability that the discrete random variable $ y $ is exactly equal to $ m $.\n",
    "\n",
    "Using the fact that all $ M $ of the probabilities must sum to one, we find:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\Pr(y = M) + \\sum_{m=0}^{M - 1} \\Pr(y = m) &= 1 \\\\\n",
    "        \\Pr(y = M) + \\sum_{m=0}^{M - 1} \\Pr(y = M)e^{Xβ_m} &= 1 & \\text{substitute equation (16)} \\\\\n",
    "        \\Pr(y = M) \\cdot (1 + \\sum_{m=0}^{M - 1} e^{Xβ_m}) &= 1 \\\\\n",
    "        \\Pr(y = M) &= \\frac{1}{1 + \\sum_{m=0}^{M - 1} e^{Xβ_m}} \\tag{17}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "And for probabilities of other classes:\n",
    "\n",
    "$$\n",
    "    \\text{for all } t = 1, \\ldots, M - 1: \\qquad\n",
    "        \\Pr(y = t) = \\frac{e^{Xβ_t}}{1 + \\sum_{m=0}^{M - 1} e^{Xβ_m}} \\tag{18}\n",
    "$$\n",
    "\n",
    "#### **Logistic loss**\n",
    "\n",
    "Finding coefficient vector $ β $ for a logistic model is somehow different. The loss function used here is usually **_logistic loss_**, which is the negative **_log-likelihood_**.\n",
    "\n",
    "The log-loss for the $ i $-th point $ \\ell_i $ is:\n",
    "\n",
    "$$\n",
    "    \\ell_i =\n",
    "        \\begin{cases}\n",
    "            - \\ln(p_i) & \\text{if } y_i = 1 \\\\\n",
    "            - \\ln(1 - p_i) & \\text{if } y_i = 0\n",
    "        \\end{cases} \\tag{19}\n",
    "$$\n",
    "\n",
    "where $ p_i = p(X_i) $, the probability of $ y_i $ equaling a success case.\n",
    "\n",
    "These two cases can be combined into a single expression:\n",
    "\n",
    "$$\n",
    "    \\ell_i = - y_i \\ln p_i - (1 - y_i) \\ln (1 - p_i) \\tag{20}\n",
    "$$\n",
    "\n",
    "This expression is more formally known as the **_cross-entropy_** of the predicted distribution $ (p_i, (1 − p_i)) $ from the actual distribution $ (y_i, (1 − y_i)) $, as probability distributions on the two-element space of (pass, fail).\n",
    "\n",
    "And the overall loss:\n",
    "\n",
    "$$\n",
    "    \\ell = \\sum_{i=1}^N (- y_i \\ln p_i - (1 - y_i) \\ln (1 - p_i)) \\tag{21}\n",
    "$$\n",
    "\n",
    "#### **Parameter estimation**\n",
    "\n",
    "Again, we need to find partial derivatives of loss function over coefficients and let them equal to zero.\n",
    "\n",
    "For a logistic model with $ K $ explanatory variables $ x_1, x_2, \\ldots, x_k $:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\ell &= \\sum_{i=1}^N (- y_i \\ln p_i - (1 - y_i) \\ln (1 - p_i)) & \\text{equation (21)}\\\\\n",
    "        \\text{for all } i = 1, \\ldots, N: \\qquad\n",
    "            p_i &= \\frac{1}{1+e^{-t_i}} \\qquad 1 - p_i = \\frac{1}{1+e^{t_i}}& \\text{equation (7)} \\\\\n",
    "        t_i &= β_0 + β_1x_{i1} + \\cdots + β_kx_{ik} & \\text{equation (8)} \\\\\n",
    "        \\text{for all } k = 1, \\ldots, K: \\qquad\n",
    "            \\frac{∂\\ell}{∂β_k} &= \\frac{∂\\ell}{∂p_i} \\cdot \\frac{∂p_i}{∂t_i} \\cdot \\frac{∂t_i}{∂β_k} & \\text{chain rule} \\\\\n",
    "        0 &= \\sum_{i=1}^N (- y_i \\cdot \\frac{1}{p_i} - (1 - y_i) (- \\frac{1}{1 - p_i}))\n",
    "            \\cdot \\frac{e^{t_i}}{(1 + e^{t_i})^2} \\cdot x_{ik} \\\\\n",
    "        0 &= \\sum_{i=1}^N (- y_i (1 + e^{-t_i}) + (1 - y_i) (1 + e^{t_i}))\n",
    "            \\cdot \\frac{e^{t_i}}{(1 + e^{t_i})^2} \\cdot x_{ik} \\\\\n",
    "        0 &= \\sum_{i=1}^N (- y_i \\cdot \\frac{1}{1 + e^{t_i}} + (1 - y_i) \\frac{e^{t_i}}{1 + e^{t_i}}) x_{ik} \\\\\n",
    "        0 &= \\sum_{i=1}^N (- y_i \\cdot (1 - p_i) + (1 - y_i) \\cdot p_i) x_{ik} \\\\\n",
    "        0 &= \\sum_{i=1}^N (p_i - y_i) x_{ik} \\tag{22}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "The maximization procedure can be accomplished by solving the equation $ (22) $ for all ⁠$ β_0, β_1, \\ldots, β_k $ by numerical methods, since $ \\ell $ is nonlinear in ⁠⁠$ β_0, β_1, \\ldots, β_k $⁠.\n",
    "\n",
    "#### **Assumptions on dataset**\n",
    "\n",
    "1. Linear separable.\n",
    "2. No collinearity between explanatory variables.\n",
    "3. Mutual (no instances are classified as more than one class)."
   ]
  },
  {
   "cell_type": "code",
   "id": "6c40f9d653af6e5",
   "metadata": {},
   "source": [
    "# use scikit-learn builtin dataset \"iris\"\n",
    "\n",
    "\"\"\"\n",
    "Builtin dataset information:\n",
    "Features: sepal length, sepal width, petal length, petal width (all in centimeters), class (target)\n",
    "Number of instances: 150 (50 per class)\n",
    "\"\"\"\n",
    "\n",
    "# load dataset\n",
    "dataframe = sklearn.datasets.load_iris(as_frame=True)\n",
    "\n",
    "# split dataset as you learnt above\n",
    "# TODO: write your code here\n",
    "\n",
    "# instantiate pipeline\n",
    "# TODO: write your code here\n",
    "\n",
    "# fit train data\n",
    "# TODO: write your code here\n",
    "\n",
    "# test accuracy (around 95%)\n",
    "# TODO: write your code here\n",
    "\n",
    "# rename classes using the iris target names\n",
    "dataframe.frame[\"target\"] = dataframe.target_names[dataframe.target]\n",
    "\n",
    "# then you can plot the dataset using pairplot\n",
    "sns.pairplot(dataframe.frame, hue=\"target\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "075adb12",
   "metadata": {},
   "source": [
    "If you forget to normalise data, you may get the following warning:\n",
    "```text\n",
    "...\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "\n",
    "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d763672be2d90",
   "metadata": {},
   "source": [
    "_Think: what's the strengths and weaknesses of logistic regression (compared to the linear regression)?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c73a9b7e49ddc8",
   "metadata": {},
   "source": [
    "_Think: when shall we use the linear regression, when shall we use the logistic regression?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52f28e6f730b95",
   "metadata": {},
   "source": [
    "# Wrap up\n",
    "\n",
    "we have covered the fundamentals of linear regression, explored its mathematical foundations, and implemented it using scikit-learn. By the end of this workshop, you should have a solid understanding of how linear models work, from data preprocessing to model evaluation. Remember, linear regression is more than just a predictive tool; it's a stepping stone to grasping more advanced machine learning techniques. Practice applying what you've learned on diverse datasets to deepen your skills and confidence. Keep experimenting, and stay curious!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
