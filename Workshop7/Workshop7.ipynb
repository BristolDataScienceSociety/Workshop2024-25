{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac8dbe4e7a4bd2d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In workshop 6, we learned linear regression and logistic regression. We talked about what kind of data works for which model, what's the purpose of each model, their assumptions on data, advantages and disadvantages. Along with introduction of models we also briefly introduced the loss functions for these models and how they work to give the best combination of parameters.\n",
    "\n",
    "However, regressions are not always good choices to use in machine learning. They rely on data preprocessing to have better accuracy and there are not many scenarios that they can be applied. In this workshop we are introducing two most popular machine learning algorithms: **decision tree** and **random forest**. They are simple but intelligent, much more powerful than regressions.\n",
    "\n",
    "So, what are they as machine learning algorithms?\n",
    "\n",
    "Imagine you're planning a vacation, and you want to decide where to go. \n",
    "\n",
    "- A **_decision tree_** is like following a single travel guidebook. It might ask questions like:  \n",
    "  - \"Do you prefer beaches or mountains?\"  \n",
    "  - \"Are you looking for a budget-friendly destination?\"  \n",
    "  - \"Is warm weather important to you?\"  \n",
    "\n",
    "  Based on your answers, it gives you a single destination, such as \"Go to Bali.\"\n",
    "\n",
    "Example of a simple decision tree:\n",
    "\n",
    "<img src=\"img/CARTClassificationAndRegressionTree.jpg\" alt=\"Simple Decision Tree\" align = \"left\" width = 1200px>\n",
    "\n",
    "\n",
    "- A **random forest**, on the other hand, is like consulting 100 travel experts. Each expert uses a slightly different guidebook or set of criteria to ask their own version of the questions.  \n",
    "  One expert might prioritize cost, while another focuses on activities, and a third considers weather. After all the experts give their suggestions, you go with the destination that most of them recommend.\n",
    "\n",
    "\n",
    "To emphasize the relationship:\n",
    "- A **decision tree** is one of the \"experts\" in the group of 100.\n",
    "- The **random forest** combines all the individual decisions (from each tree) into one stronger and more reliable conclusion.  \n",
    "\n",
    "\n",
    "Decision trees and random forests have transformed machine learning by offering a balance of power and practicality. Decision trees made machine learning more accessible and interpretable - even non-experts can understand how they make decisions, making them valuable in fields like medical diagnosis and credit scoring where transparency is crucial. Random forests then solved the main weakness of decision trees (their tendency to overfit) while maintaining most of their interpretability, becoming one of the most reliable and widely-used algorithms in data science. Together, they've shown that machine learning doesn't always need to be a complex \"black box\" to be effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635384de",
   "metadata": {},
   "source": [
    "# Decision Tree Learning\n",
    "\n",
    "Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.\n",
    "\n",
    "Features can be classified as categorical features or numerical features. Categorical features can take on one of a limited number of possible values, while numerical features can take any integers or real numbers (sometimes are restricted to a range).\n",
    "\n",
    "Categorical features are always discrete. Numerical features can be either continuous or discrete, depending on their data types.\n",
    "\n",
    "There are three well-known decision tree algorithms: ID3, C4.5 and CART."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6d6e9866199ea",
   "metadata": {},
   "source": [
    "## 1. Information theory\n",
    "\n",
    "Constructing a decision tree is dividing dataset at each node. A good decision tree is a tree that there are strong distinctions between subsets assigned to children nodes.\n",
    "\n",
    "\n",
    "### Entropy\n",
    "**_Entropy_** $ H(T) $ describes the uncertainty of a target. Define random discrete variable $ T \\in {\\cal T} $.\n",
    "\n",
    "$$\n",
    "    H(T) = - \\sum_{t \\in {\\cal T}} \\Pr(T = t) \\log_2 \\Pr(T = t) \\tag{1}\n",
    "$$\n",
    "\n",
    "where $ P $ stands for **_probability mass function_** and $ P(T = t) $ is the probability that $ T $ is exactly equal to $ t $.\n",
    "\n",
    "### Information Gain\n",
    "**_Information Gain_** $ IG(T, A) $ measures how much uncertainty (entropy) is reduced about the target variable $ T $ after splitting the data by the attribute $ A $, thus, the entropy difference between a parent node and sum of its children nodes.\n",
    "\n",
    "Let $ T $ be a target random variable with a discrete set of possible outcomes $ \\mathcal{T} = \\{t_1, t_2, \\dots, t_k\\} $, and let $ A $ be an attribute with a discrete set of possible values $ \\mathcal{A} = \\{a_1, a_2, \\dots, a_m\\} $. The information gain $ IG(T, A) $, which quantifies the reduction in uncertainty about $ T $ after splitting on $ A $, is defined as:\n",
    "\n",
    "$$\n",
    "IG(T, A) = H(T) - H(T \\mid A)\\tag{2},\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "1. **Entropy of $ T $:**\n",
    "$$\n",
    "H(T) = -\\sum_{t \\in \\mathcal{T}} P(T = t) \\log_2 P(T = t),\n",
    "$$\n",
    "\n",
    "2. **Conditional Entropy of $ T $ given $ A $:**\n",
    "$$\n",
    "H(T \\mid A) = \\sum_{a \\in \\mathcal{A}} P(A = a) \\left( -\\sum_{t \\in \\mathcal{T}} P(T = t \\mid A = a) \\log_2 P(T = t \\mid A = a) \\right).\n",
    "$$\n",
    "    \n",
    "where $ P(T = t | A = a) $ is the conditional probability that the variable $ T $ is exactly equal to $ t $ given that variable $ A $ is exactly equal to $ a $.\n",
    "\n",
    "\n",
    "Thus, the information gain can be explicitly expressed as:\n",
    "\n",
    "$$\n",
    "IG(T, A) = -\\sum_{t \\in \\mathcal{T}} P(T = t) \\log_2 P(T = t)\n",
    "+ \\sum_{a \\in \\mathcal{A}} P(A = a) \\sum_{t \\in \\mathcal{T}} P(T = t \\mid A = a) \\log_2 P(T = t \\mid A = a)\\tag{3}.\n",
    "$$\n",
    "\n",
    "\n",
    "where $ P(T = t | A = a) $ is the conditional probability that the variable $ T $ is exactly equal to $ t $ given that variable $ A $ is exactly equal to $ a $.\n",
    "\n",
    "The conditional entropy $ H(T|A) $ is the weighted sum of entropies of children nodes, so we need a summation over all possible values of attribute $ A $ because we split data by it. $ P(A = a) $ makes the entropy weighted and the remaining part is basically the entropy when stand at views of children nodes, because these nodes are processing data already split by parent node so the probability distribution of target $ T $ changes.\n",
    "\n",
    "Assuming that we have a pack of candies, some of them are tasty but the others are tasteless. We would like to know which attributes of those candies bring relevant information of the tasteness of the candy by calculating the information gain. Below is the table describing the appearance of some candies (tasteless candies are indicated in the table, others are tasty):\n",
    "\n",
    "| Size\\Colour | Red | Green | TOTAL |\n",
    "| --- | :---: | :---: | :---: |\n",
    "| Small | 5 | 10 (5 tasteless) | 15 (5 tasteless) |\n",
    "| Normal | 15 | 20 (8 tasteless) | 35 (8 tasteless) |\n",
    "| TOTAL | 20 | 30 (13 tasteless) | 50 (13 tasteless) |\n",
    "\n",
    "Assume that we are classifying candies by their tastiness, then the entropy of root node is\n",
    "\n",
    "$$\n",
    "    {\\rm H}(\\text{Tastiness}) = - (\\frac{13}{50} \\log_2 \\frac{13}{50} + \\frac{37}{50} \\log_2 \\frac{37}{50}) = 0.827\n",
    "$$\n",
    "\n",
    "Conditional entropies of children nodes:\n",
    "\n",
    "$$\n",
    "    {\\rm H}(\\text{Tastiness} | \\text{Colour}) =\n",
    "        \\overbrace{-\\frac{20}{50} (\\underbrace{\\frac{20}{20} \\log_2 \\frac{20}{20}}_\\text{tasty} + \\underbrace{\\frac{0}{20} \\log_2 \\frac{0}{20}}_\\text{tasteless})}^\\text{red} +\n",
    "        \\overbrace{-\\frac{30}{50} (\\underbrace{\\frac{17}{30} \\log_2 \\frac{17}{30}}_\\text{tasty} + \\underbrace{\\frac{13}{30} \\log_2 \\frac{13}{30}}_\\text{tasteless})}^\\text{green}\n",
    "    = 0.592\n",
    "$$\n",
    "\n",
    "$$\n",
    "    {\\rm H}(\\text{Tastiness} | \\text{Size}) =\n",
    "        \\overbrace{-\\frac{15}{50} (\\underbrace{\\frac{10}{15} \\log_2 \\frac{10}{15}}_\\text{tasty} + \\underbrace{\\frac{5}{15} \\log_2 \\frac{5}{15}}_\\text{tasteless})}^\\text{small} +\n",
    "        \\overbrace{-\\frac{35}{50} (\\underbrace{\\frac{27}{35} \\log_2 \\frac{27}{35}}_\\text{tasty} + \\underbrace{\\frac{8}{35} \\log_2 \\frac{8}{35}}_\\text{tasteless})}^\\text{normal}\n",
    "    = 0.818\n",
    "$$\n",
    "\n",
    "And information gains when splitted by either colour or size:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        IG(\\text{Tastiness}, \\text{Colour}) &= {\\rm H}(\\text{Tastiness}) - {\\rm H}(\\text{Tastiness}|\\text{Colour})\n",
    "            = 0.827 - 0.592 = 0.235 \\\\\n",
    "        IG(\\text{Tastiness}, \\text{Size}) &= {\\rm H}(\\text{Tastiness}) - {\\rm H}(\\text{Tastiness}|\\text{Size})\n",
    "            = 0.827 - 0.818 = 0.009 \\\\\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "$ 0.235 > 0.009 $, which means splitting by the colour attribute brings more information about the data, because from the dataset we know all the tasteless candies are green, then splitting by colour removes the uncertainty on the red candies (they are all tasty). But splitting by size you are still uncertain about the tastiness of two sets of candies.\n",
    "\n",
    "*Although being very useful, the Information Gain has an undesired characteristic, which is to favor the predictor variables with a large number of values. Those highly branching predictors are likely to split the data into subsets with low Entropy values.*\n",
    "\n",
    "\n",
    "### Gain Ratio (Normalized Information Gain)\n",
    "\n",
    "**_Gain Ratio_** attempts to lessen the bias of **_Information Gain_** on highly branched predictors by introducing a normalizing term called the **_Intrinsic Information_**.\n",
    "\n",
    "\n",
    "1. Intrinsic Information (II)\n",
    "\n",
    "    The **Intrinsic Information** (or **Intrinsic Value**) is a measure of the entropy of sub-dataset proportions. It reflects the difficulty of guessing which branch a randomly selected sample belongs to after a split. The formula is:\n",
    "$$\n",
    "II = -\\sum_{i=1}^n \\frac{|S_i|}{|S|} \\log_2\\left(\\frac{|S_i|}{|S|}\\right)\n",
    "$$\n",
    "    Where:\n",
    "    - $ n $ = the number of subsets (branches) formed by the split.\n",
    "    - $ |S_i| $ = the size of the \\(i\\)-th subset.\n",
    "    - $|S|$ = the size of the parent dataset.\n",
    "\n",
    "\n",
    "2. Gain Ratio\n",
    "\n",
    "    The **Gain Ratio** adjusts the **Information Gain (IG)** by dividing it by the Intrinsic Information. The formula is:\n",
    "$$\n",
    "\\text{Gain Ratio} = \\frac{\\text{Information Gain (IG)}}{\\text{Intrinsic Information (II)}}\n",
    "$$\n",
    "    Where:\n",
    "    - $ IG $ = Information Gain, which measures how much the split reduces the entropy of the dataset.\n",
    "    - $ II $ = Intrinsic Information, which normalizes the Information Gain.\n",
    "\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "**_Gini Impurity_** $ {\\rm I}_G(p)$ is the probability of misclassification, assuming independent selection of the element and its class based on the class probabilities. \n",
    "\n",
    "For a set of items with $ M $ classes and relative frequencies $ p_i, i \\in { 1, 2, \\ldots, M } $, the probability of choosing an item with label $ i $ is $ p_i $ and the probability of miscategorizing that item is $ 1 - p_i $. The Gini impurity is computed by summing pairwise products of these probabilities for each class lable:\n",
    "\n",
    "$$\n",
    "    {\\rm I}_G(p) = \\sum_{i=1}^M p_i(1 - p_i) = \\sum_{i=1}^M (p_i - p_i^2) = \\sum_{i=1}^M p_i - \\sum_{i=1}^M p_i^2\n",
    "        = 1 - \\sum_{i=1}^M p_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02d53a",
   "metadata": {},
   "source": [
    "## 2. Iterative Dichotomiser 3 (ID3)\n",
    "\n",
    "The ID3 algorithm begins with the original set $ S $ as the root node. On each iteration of the algorithm, it iterates through every unused attribute of the set $ S $ and calculates the entropy $ {\\rm H}(S) $ or the information gain $ IG(S) $ of that attribute. It then selects the attribute which has the smallest entropy (or largest information gain) value. The set $ S $ is then split or partitioned by the selected attribute to produce subsets of the data.\n",
    "\n",
    "Recursion on a subset may stop in one of these cases:\n",
    "\n",
    "- every element in the subset belongs to the same class; in which case the node is turned into a leaf node and labelled with the class of the examples.\n",
    "- there are no more attributes to be selected, but the examples still do not belong to the same class. In this case, the node is made a leaf node and labelled with the most common class of the examples in the subset.\n",
    "- there are no examples in the subset, which happens when no example in the parent set was found to match a specific value of the selected attribute. An example could be the absence of a person among the population with age over 100 years. Then a leaf node is created and labelled with the most common class of the examples in the parent node's set.\n",
    "\n",
    "Throughout the algorithm, the decision tree is constructed with each non-terminal node (internal node) representing the selected attribute on which the data was split, and terminal nodes (leaf nodes) representing the class label of the final subset of this branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7c550",
   "metadata": {},
   "source": [
    "## 3. C4.5\n",
    "\n",
    "C4.5 builds decision trees from a set of training data in the same way as ID3, using the concept of information entropy. The training data is a set $ S = s_1, s_2, \\ldots $ of already classified samples. Each sample $ s_i $ consists of a $ k $-dimensional vector $ \\begin{bmatrix} x_{i1} & x_{i2} & \\cdots & x_{ik} \\end{bmatrix} $, where the $ x_{ij}, j \\in K $ represent attribute values or features of the sample, as well as the class in which $ s_i $ falls.\n",
    "\n",
    "At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the gain ratio. The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurses on the partitioned sublists.\n",
    "\n",
    "This algorithm has a few base cases.\n",
    "\n",
    "- All the samples in the list belong to the same class. When this happens, it simply creates a leaf node for the decision tree saying to choose that class.\n",
    "- None of the features provide any information gain. In this case, C4.5 creates a decision node higher up the tree using the expected value of the class.\n",
    "- Instance of previously unseen class encountered. Again, C4.5 creates a decision node higher up the tree using the expected value.\n",
    "\n",
    "In pseudocode, the general algorithm for building decision trees is:\n",
    "\n",
    "1. Check for the above base cases.\n",
    "2. For each attribute $ a $, find the gain ratio from splitting on $ a $.\n",
    "3. Let $ a\\_best $ be the attribute with the highest gain ratio.\n",
    "4. Create a decision node that splits on $ a\\_best $.\n",
    "5. Recurse on the sublists obtained by splitting on $ a\\_best $, and add those nodes as children of node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc9ff5",
   "metadata": {},
   "source": [
    "## 4. Classification and Regression Trees (CART)\n",
    "\n",
    "CART is an umbrella term consisting of classification tree algorithm and regressor tree algorithm.\n",
    "\n",
    "Unlike ID3 and C4.5 which can create multi-way splits for categorical features, CART always creates binary splits. For categorical task with $ M $ classes, CART considers all $ 2^{M-1}-1 $ possible binary groupings. For continuous variables, it sorts values and consider all possible threshold splits.\n",
    "\n",
    "For classification task, CART uses Gini impurity instead of information gain since Gini impurity is less computationally intensive than entropy. For regression task, the usual choice is mean squared error (see workshop 6).\n",
    "\n",
    "When this algorithm reaches some termination criteria, we come up with a maximum tree, which typically overfits with the training data. Pruning is used to remove sections of the tree that are non-critical and redundant to classify instances and hence improves predictive accuracy.\n",
    "\n",
    "**_Reduced error pruning_** is one of the simplest forms of pruning. Starting at the leaves, each node is replaced with its most popular class. If the prediction accuracy is not affected then the change is kept. While somewhat naive, reduced error pruning has the advantage of simplicity and speed.\n",
    "\n",
    "**_Cost complexity pruning_** generates a series of trees ⁠$ T_0, \\ldots, T_m $⁠ where ⁠$ T_0 $ is the initial tree and ⁠$ T_n $⁠ is the root alone. At step ⁠$ i = 1, \\ldots, n $⁠, the tree is created by removing a subtree $ t $ from tree ⁠$ T_{i-1} $⁠ and replacing it with a leaf node with value chosen as in the tree building algorithm. The subtree $ t $ that is removed is chosen with minimum relative accuracy loss, as follows:\n",
    "\n",
    "$$\n",
    "    \\underset{t}{\\rm argmin} \\left( \\frac{{\\rm accuracy}(T, S) - {\\rm accuracy}(T - t, S)}{|T| - |T - t|} \\right)\n",
    "$$\n",
    "\n",
    "where $ {\\rm argmin} $ returns a subtree $ t $ where the expression minimized, $ {\\rm accuracy}(T, S) $ evaluates to accuracy of tree $ T $ on dataset $ S $, $ T - t $ means the remaining tree when subtree $ t $ is removed from $ T $ and $ |T| $ is the number of leaves in tree $ T $.\n",
    "\n",
    "Once the series of trees ⁠$ T_0, \\ldots, T_m $ has been created, the best tree is chosen by generalized accuracy as measured by a training set or cross-validation.\n",
    "\n",
    "\n",
    "**Differences among ID3, C4.5, and CART**\n",
    "\n",
    "| Feature          | ID3  | C4.5 | CART  |\n",
    "|-----------------|------|------|-------|\n",
    "| **Splitting Criterion** | Information Gain | Gain Ratio | Gini Impurity (Classification) / MSE (Regression) |\n",
    "| **Type of Splits** | Multi-way | Multi-way | **Binary** |\n",
    "| **Handles Continuous Attributes?** | No | Yes | Yes |\n",
    "| **Handles Missing Values?** | No | Yes | Yes |\n",
    "| **Pruning** | No | Yes (Post-Pruning) | Yes (Cost Complexity Pruning) |\n",
    "| **Output Type** | Classification Tree | Classification Tree | Classification & Regression Tree |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee87df05188250",
   "metadata": {},
   "source": [
    "## 5. The Forest CoverType dataset\n",
    "\n",
    "This dataset contains both categorical and numerical data, is a classification task.\n",
    "\n",
    "Number of instances: 581,012\n",
    "\n",
    "Number of attributes: 12 (dimensionality: 54)\n",
    "\n",
    "Target: Cover_Type (integer)\n",
    "\n",
    "Feature Table:\n",
    "\n",
    "| Index | Name | Type |\n",
    "| ---: | --- | --- |\n",
    "| 0 | Elevation | Integer |\n",
    "| 1 | Aspect | Integer |\n",
    "| 2 | Slope | Integer |\n",
    "| 3 | Horizontal_Distance_To_Hydrology | Integer |\n",
    "| 4 | Vertical_Distance_To_Hydrology | Integer |\n",
    "| 5 | Horizontal_Distance_To_Roadways | Integer |\n",
    "| 6 | Hillshade_9am | Integer |\n",
    "| 7 | Hillshade_Noon | Integer |\n",
    "| 8 | Hillshade_3pm | Integer |\n",
    "| 9 | Horizontal_Distance_To_Fire_Points | Integer |\n",
    "| 10-13 | Wilderness_Area | Categorical (one-hot) |\n",
    "| 14-53 | Soil_Type | Categorical (one-hot) |\n",
    "\n",
    "**_One-hot_** is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0).\n",
    "\n",
    "Use [`sklearn.datasets.fetch_covtype`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html) to load the covertype dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "3c325847a951a780",
   "metadata": {},
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# load covertype dataset\n",
    "X, y = sklearn.datasets.fetch_covtype(as_frame=True, return_X_y=True)\n",
    "\n",
    "# split the dataset into 80% train set and 20% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(f\"{X.shape = }\")\n",
    "print(f\"example of Wilderness_Area: {np.asarray(X)[0][10:14]}\")\n",
    "print(f\"example of Soil_Type: {np.asarray(X)[0][14:]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "73d96737d95a7580",
   "metadata": {},
   "source": [
    "## 6. Workshop 6 review task\n",
    "\n",
    "_If you didn't come to workshop 6 or you don't know linear regression, you can skip this exercise._\n",
    "\n",
    "Use logistic regression to do the classification task, train you model with train set, and then test accuracy on the test set.\n",
    "\n",
    "You might get convergence warning with the default number of iterations (100). There is no need to try with a higher setting since it takes significant time to train the model but the model parameters may still cannot reaches convergence and does not provide higher accuracy. This is the limitation of logistic model.\n",
    "\n",
    "_Think: how would you preprocess the data?_"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# step 1: INITIALISE and FIT logistic regression model without preprocessing\n",
    "# model = \n",
    "\n",
    "print(f\"no preprocessing: {model.score(X_test, y_test)}\")\n",
    "\n",
    "# step 2: initialise a pipeline with normalisation and logistic regression\n",
    "# TODO: write your code here\n",
    "# hint: pipe = \n",
    "\n",
    "# step 3: fit the pipline\n",
    "# TODO: write your code here\n",
    "\n",
    "print(f\"with normalisation: {pipe.score(X_test, y_test)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2eab438d1963ad0b",
   "metadata": {},
   "source": [
    "## 7. CART for classification\n",
    "\n",
    "Use classification tree to do the covertype task.\n",
    "\n",
    "Use [`sklearn.tree.DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to build a classification tree. The following is the signature of the class:\n",
    "\n",
    "```python\n",
    "class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None)\n",
    "```\n",
    "\n",
    "The star symbol (`*`) as the first argument indicates that all the arguments followed can only be set by keyword arguments (e.g. call `sklearn.tree.DecisionTreeClassifier(max_depth=10)` instead of `sklearn.tree.DecisionTreeClassifier(10)`).\n",
    "\n",
    "The `criterion` argument sets the loss function to use, the default is gini impurity, available choices are `'gini'`, `'entropy'` and `'log_loss'`.\n",
    "\n",
    "The `max_depth` argument sets the maximum depth of tree, which helps to avoid overfitting on training set. The default is `None` which does not restrict tree depth."
   ]
  },
  {
   "cell_type": "code",
   "id": "d28ceb4011bee113",
   "metadata": {},
   "source": [
    "# step1: initialize decision tree classifier\n",
    "# TODO: write your code here\n",
    "# model = \n",
    "\n",
    "# step2: fit decision tree classifier\n",
    "# TODO: write your code here\n",
    "\n",
    "# step3: evaluate the decision tree model\n",
    "# model.score(X_test, y_test) # uncomment this line to evaluate the model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "acae22c536b33874",
   "metadata": {},
   "source": [
    "_Think: compared to logistic regression, which has a higher accuracy in this prediction task? What causes the differences?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746c84cbbf815c1",
   "metadata": {},
   "source": [
    "## 8. CART for regression\n",
    "\n",
    "Use [`sklearn.tree.DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) to build a regression tree.\n",
    "\n",
    "The fitting result of a regression tree is completely different from that of linear regression. In linear regression we assume there exists a linear relationship between features and target, so the model is actually a smooth best-fit curve. While in regression tree, the domain of each feature is splitted and each part is mapped with a target value.\n",
    "\n",
    "The following code piece shows the difference between linear regression and regression tree."
   ]
  },
  {
   "cell_type": "code",
   "id": "82411b07de279eb9",
   "metadata": {},
   "source": [
    "# use Workshop6/data/simple_linear.csv\n",
    "\n",
    "dataset = pd.read_csv(\"../Workshop6/data/simple_linear.csv\").to_numpy()\n",
    "X, y = np.split(dataset, [1], axis=1)\n",
    "\n",
    "model1 = sklearn.linear_model.LinearRegression().fit(X, y)\n",
    "model2 = sklearn.tree.DecisionTreeRegressor().fit(X, y)\n",
    "\n",
    "plt.ylabel(\"Physics score\")\n",
    "plt.xlabel(\"Math score\")\n",
    "plt.scatter(X, y, s=15, c=\"r\")\n",
    "plt.plot([0, 100], model1.predict([[0], [100]]), label=\"Linear regression\")\n",
    "x_samples = np.linspace(0, 100, 1000)\n",
    "plt.plot(x_samples, model2.predict([[x] for x in x_samples]), label=\"Regression tree\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd9d20ad93edfe3c",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "In workshop 6, we have learnt the concepts of **_underfitting_** and **_overfitting_**, and demonstrated how to choose the degree in polynomial regression by experimenting the performance of each choice.\n",
    "\n",
    "Obviously, the regression tree example in previous section overfits since the line passes through many data points. This can be improved by reduce the maximum depth, which is a hyperparameter of regression tree.\n",
    "\n",
    "A **_hyperparameter_** is an attribute of model that can be set in order to define any configurable part of a model's learning process. They must be set before training. Don't be confused with **_parameters_**, which are characteristics that the model learns from the data.\n",
    "\n",
    "There are several ways to optimise hyperparameters of your models: grid search, random search, Bayesian optimisation, gradient-based optimisation, evolutionary optimisation, etc. The procedure used in workshop 6 is basically the idea of grid search.\n",
    "\n",
    "**_Cross-validation_** is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. Cross-validation includes resampling and sample splitting methods that use different portions of the data to test and train a model on different iterations. It is often used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. It can also be used to assess the quality of a fitted model and the stability of its parameters.\n",
    "\n",
    "One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the **_validation set_**). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.\n",
    "\n",
    "Diagram of **_k-fold cross-validation_**: \n",
    "\n",
    "<img src=\"img/K-fold_cross_validation.png\" alt=\"K-fold cross-validation\" height=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52619745770032a",
   "metadata": {},
   "source": [
    "## 1. Grid search\n",
    "\n",
    "Grid search is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a hold-out validation set.\n",
    "\n",
    "Use [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) in scikit-learn to do a grid search. The first argument is the model your are using, and the second one is a dictionary of what hyperparameters you are testing mapped with their possible values. The `cv_results_` attribute is a dictionary of cross-validation results, and the `best_estimator_` attribute is the model instantiated with the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "210b2d19460cf780",
   "metadata": {},
   "source": [
    "# use data/polynomial.csv in workshop6\n",
    "\n",
    "# step1: Load and prepare the data\n",
    "# Split the dataset into features (X) and target values (y).\n",
    "dataset = pd.read_csv(\"../Workshop6/data/polynomial.csv\").to_numpy()\n",
    "X, y = np.split(dataset, [1], axis=1)\n",
    "\n",
    "# step2: Split the data into 80% train set and 20% test set\n",
    "# TODO: write your code here\n",
    "\n",
    "# step3: Define the pipeline and perform hyperparameter tuning\n",
    "# Create a pipeline that includes polynomial feature transformation and a linear regression model.\n",
    "# Use GridSearchCV to find the best degree for PolynomialFeatures.\n",
    "# TODO: write your code here\n",
    "# pipe = \n",
    "# grid = \n",
    "\n",
    "# step4: Fit the pipeline with training data\n",
    "# TODO: write your code here\n",
    "\n",
    "print(f\"accuracy: {grid.score(X_test, y_test)}\") # usually around 99%\n",
    "print(f\"best: {grid.best_estimator_}\") # degree of PolynomialFeatuers is usually 2 or 3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "59adcb4cfa142f6f",
   "metadata": {},
   "source": [
    "## 2. Bayesian optimisation\n",
    "\n",
    "Bayesian optimization is typically used on problems of the form $ \\max_{x \\in {\\cal A}}f(x) $, where $ {\\cal A} $ is a set of points, $ x $, which rely upon less than (or equal to) 20 dimensions ($ {\\Bbb R}^d, d \\le 20 $), and whose membership can easily be evaluated. Bayesian optimization is particularly advantageous for problems where f(x) is difficult to evaluate due to its computational cost. The objective function, $ f $, is continuous and takes the form of some unknown structure, referred to as a \"black box\". Upon its evaluation, only $ f(x) $ is observed and its derivatives are not evaluated.\n",
    "\n",
    "Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines the next query point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bf34b479b5e2c",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "**_Random Forest_** is an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the output is the average of the predictions of the trees. Random forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "**_Ensemble methods_** are methods that uses the idea of combining multiple learning algorithms together to make a more reliable prediction than using any of them in themselves.\n",
    "\n",
    "## 1. Bootstrap aggregating\n",
    "\n",
    "The training algorithm for random forests applies the general technique of **_bootstrap aggregating_**, or **_bagging_**, to tree learners. Given a training set, bagging repeatedly ($ B $ times) selects a random sample with replacement of the training set and fits trees to these samples.\n",
    "\n",
    "After training, regression predictions for unseen samples $ x' $ can be made by averaging the predictions from all the individual regression trees on $ x' $:\n",
    "\n",
    "$$\n",
    "    \\hat{f} = \\frac 1B \\sum_{b=1}^B f_b (x')\n",
    "$$\n",
    "\n",
    "where $ \\hat{f} $ is the final prediction function, $ f_b $ is the prediction function of $ b $-th tree.\n",
    "\n",
    "For classification tasks, prediction is made by taking the plurality vote.\n",
    "\n",
    "This bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\n",
    "\n",
    "Use [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to do a classification task. The `n_estimators` argument specifies number of decision trees, defaults to 100. The `bootstrap` argument specifies whether bootstrap samples are used when building trees, defaults to `True`."
   ]
  },
  {
   "cell_type": "code",
   "id": "ed94442565942b27",
   "metadata": {},
   "source": [
    "# step1: Load the Forest CoverType dataset\n",
    "# Fetch the dataset and split it into features (X) and target (y).\n",
    "# TODO: write your code here\n",
    "# X, y =\n",
    "\n",
    "# step2: Split the data into 80% train set and 20% test set\n",
    "# TODO: write your code here\n",
    "\n",
    "# step3: Train a RandomForestClassifier\n",
    "# Instantiate the model and fit it with the training data.\n",
    "# TODO: write your code here\n",
    "\n",
    "# step4: evaluate the decision tree model (usually around 95%)\n",
    "# TODO: write your code here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a06ff8ab00725dc5",
   "metadata": {},
   "source": [
    "_Think: compared to a single decision tree, which has a higher accuracy in this prediction task? What do you think is the reason behind this? Would it always hold in all tasks and situations?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600fc9f2d85e3754",
   "metadata": {},
   "source": [
    "# Optional Task\n",
    " \n",
    "Use digits dataset ([`sklearn.datasets.load_digits`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)) to do a classification task and compare accuracies of decision trees by using preprocessed data and not-preprocessed data. Is the difference significant and why is that?"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe45e3a731dfec42",
   "metadata": {},
   "source": [
    "# step1: Load the Digits dataset\n",
    "# TODO: write your code here\n",
    "# X, y = \n",
    "\n",
    "# step2: Split the data into 80% train set and 20% test set\n",
    "# TODO: write your code here\n",
    "\n",
    "# Step3: Train and evaluate a DecisionTreeClassifier without preprocessing\n",
    "# TODO: write your code here\n",
    "# model1 = \n",
    "\n",
    "print(f\"accuracy 1: {model1.score(X_test, y_test)}\")\n",
    "\n",
    "# Step4: Train and evaluate a DecisionTreeClassifier with StandardScaler preprocessing\n",
    "# TODO: write your code here\n",
    "# model2 = \n",
    "\n",
    "print(f\"accuracy 2: {model2.score(X_test, y_test)}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
