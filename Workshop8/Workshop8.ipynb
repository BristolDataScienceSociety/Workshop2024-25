{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718efb0b6d150868",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is our last workshop, in this workshop, we'll focus on a popular and powerful dimensionality reduction method called Principle Component Analysis (PCA), followed by a brief introduction about neural networks, taking multi-layer perceptron (MLP) as an instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6f7e8f29f200e",
   "metadata": {},
   "source": [
    "# Principle Component Analysis (PCA)\n",
    "\n",
    "## 1. Curse of dimensionality\n",
    "\n",
    "The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.\n",
    "\n",
    "Filling a lines between $ 0 $ and $ 1 $ where maximum separation between adjacent points is $ 0.1 $, then we need at least $ 11 $ points. As dimension goes up to 2, which is a plane, we need $ 11^2 $ points. In machine learning we usually have datasets with tens of features, we need $ 11^n $ points to ensure maximum separation is still $ 0.1 $.\n",
    "\n",
    "However, most datasets only have thousands of data points. If you can imagine, these data points in high-dimensional spaces are far away. When the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties, but in high-dimensional data, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.\n",
    "\n",
    "![curse of dimensionality](img/Curse-of-Dimensionality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb3d361c3f07c0",
   "metadata": {},
   "source": [
    "## 2. Reduction of dimensionality\n",
    "\n",
    "PCA is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing.\n",
    "\n",
    "The data is linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified.\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [-3, -1, 1, 3]\n",
    "y = [-1.5, -0.5, 0.5, 1.5]\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-2, 2)\n",
    "plt.scatter(x, y)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "ax.spines[\"right\"].set_color(\"none\")\n",
    "ax.spines[\"top\"].set_color(\"none\")\n",
    "ax.xaxis.set_ticks_position(\"bottom\")\n",
    "ax.spines[\"bottom\"].set_position((\"data\", 0))\n",
    "ax.yaxis.set_ticks_position(\"left\")\n",
    "ax.spines[\"left\"].set_position((\"data\", 0))\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95733766",
   "metadata": {},
   "source": [
    "Obviously these points can make up a straight line. If we rotate the axes to make x-axis lies on these points, then all points have a zero values of y, and we can remove that axis to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "id": "566f80df",
   "metadata": {},
   "source": [
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-2, 2)\n",
    "plt.scatter(x, y)\n",
    "plt.arrow(-4, -2, 8, 4)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "ax.spines[\"right\"].set_color(\"none\")\n",
    "ax.spines[\"top\"].set_color(\"none\")\n",
    "ax.spines[\"bottom\"].set_color(\"none\")\n",
    "ax.spines[\"left\"].set_color(\"none\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2cdb542db98e97a9",
   "metadata": {},
   "source": [
    "The core idea behind PCA is to project data onto a new set of orthogonal axes such that the first axis (first principal component) captures the most variance, the second captures the next most variance, and so on. This allows us to remove redundant information, reduce computational complexity, and improve visualization while minimizing the loss of important information.\n",
    "\n",
    "**Principal Components**:\n",
    "Principal components are the new axes formed by eigenvectors. They are ranked based on their eigenvalues, with the first principal component capturing the most variance.\n",
    "Mathematically, the first principal component is the eigenvector corresponding to the largest eigenvalue.\n",
    "\n",
    "![principle axes](img/axes.png)\n",
    "\n",
    "**Principal Axes**: the first direction that accounts for as much of the variance as possible (à i.e. variance is maximum); then the direction orthogonal to the first for which the variance is maximum, and so on…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99eddbb8cbcc5a4",
   "metadata": {},
   "source": [
    "## 3. Assumptions of PCA\n",
    "\n",
    "- **Linearity**: PCA assumes that the data can be well represented by a linear combination of features, meaning nonlinear relationships are not captured.\n",
    "\n",
    "- **Large Variance** = Important Information: It assumes that principal components (PCs) capturing the most variance also represent the most meaningful patterns in the data.\n",
    "\n",
    "- **Mean-Centered Data**: PCA assumes the data is centered (zero mean) to ensure that the first principal component captures the direction of maximum variance correctly.\n",
    "\n",
    "- **Independent Principal Components**: The resulting principal components are uncorrelated (orthogonal), meaning each component captures unique variance in the data.\n",
    "\n",
    "- **Gaussian Distribution (Optional)**: While not a strict requirement, PCA works best when features follow a normal distribution since variance is a good measure of information in such cases.\n",
    "\n",
    "- **Equal Importance of Feature Scaling**: PCA assumes that features are on a comparable scale. If not standardized, features with larger numerical ranges dominate variance.\n",
    "\n",
    "- **No Significant Outliers**: PCA is sensitive to outliers because it is based on variance; extreme values can distort the principal components.\n",
    "\n",
    "These assumptions impact the effectiveness of PCA, and violating them can lead to misleading results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450e670",
   "metadata": {},
   "source": [
    "## 4. Mathematical derivation\n",
    "\n",
    "PCA is finding axes where main axis towards maximum variance and has lowest average squared perpendicular distance from data points to the main axis.\n",
    "\n",
    "For a dataset $ D $ of normally distributed x-y pair, we can apply two linear transformations $ S $ (scaling) and $ R $ (rotation), to make it looks like a normalised data set $ D' $ we usually have, where\n",
    "\n",
    "$$\n",
    "    D =\n",
    "        \\begin{bmatrix}\n",
    "            x_1 & x_2 & \\cdots & x_n \\\\\n",
    "            y_1 & y_2 & \\cdots & y_n\n",
    "        \\end{bmatrix} \\qquad\n",
    "\n",
    "    S = \\begin{bmatrix}\n",
    "            a & 0 \\\\\n",
    "            0 & b\n",
    "        \\end{bmatrix} \\qquad\n",
    "\n",
    "    R = \\begin{bmatrix}\n",
    "            \\cos θ & -\\sin θ \\\\\n",
    "            \\sin θ & \\cos θ\n",
    "        \\end{bmatrix} \\qquad\n",
    "$$\n",
    "\n",
    "$$\n",
    "    D' = RSD \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9cf552c546eec21",
   "metadata": {},
   "source": [
    "x = np.random.normal(0, 1, 500)\n",
    "y = np.random.normal(0, 1, 500)\n",
    "\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.gca().set_aspect(1)\n",
    "plt.scatter(x, y, edgecolors=\"black\")\n",
    "plt.arrow(0, 0, 1, 0, color=\"red\")\n",
    "plt.arrow(0, 0, 0, 1, color=\"yellow\")\n",
    "plt.title(\"Normally distributed dataset\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64ae842f",
   "metadata": {},
   "source": [
    "D = np.column_stack([x, y]).T\n",
    "S = np.array([[2, 0], [0, 1]])\n",
    "ANGLE = math.radians(45)\n",
    "R = np.array([[math.cos(ANGLE), -math.sin(ANGLE)], [math.sin(ANGLE), math.cos(ANGLE)]])\n",
    "D2 = R @ S @ D\n",
    "\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.gca().set_aspect(1)\n",
    "plt.scatter(D2[0], D2[1], edgecolors=\"black\")\n",
    "plt.arrow(0, 0, math.cos(ANGLE) * 2, math.sin(ANGLE) * 2, color=\"red\")\n",
    "plt.arrow(0, 0, -math.sin(ANGLE), math.cos(ANGLE), color=\"yellow\")\n",
    "plt.title(\"Dataset we usually have\")\n",
    "plt.show()\n",
    "\n",
    "print(np.cov(D2[0], D2[1]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42ca35ec",
   "metadata": {},
   "source": [
    "When we transform white data $ D $ to dataset we have $ D' $, we can say the direction of stretching is the axis where variance is largest, and $ R $ rotate that axis by a specific angle. Our goal is to find $ R $ based on our dataset, so we can determine axes for PCA.\n",
    "\n",
    "$$\n",
    "    D' = RSD \\tag{1}\n",
    "$$\n",
    "$$\n",
    "    D = S^{-1}R^{-1}D' \\tag{2}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    S^{-1} = \\begin{bmatrix}\n",
    "            1/a & 0 \\\\\n",
    "            0 & 1/b\n",
    "        \\end{bmatrix} \\qquad\n",
    "\n",
    "    R^{-1} = \\begin{bmatrix}\n",
    "            \\cos -θ & -\\sin -θ \\\\\n",
    "            \\sin -θ & \\cos -θ\n",
    "        \\end{bmatrix}\n",
    "        = \\begin{bmatrix}\n",
    "            \\cos θ & \\sin θ \\\\\n",
    "            -\\sin θ & \\cos θ\n",
    "        \\end{bmatrix} = R^{\\rm T}\n",
    "$$\n",
    "\n",
    "Then how can we find $ R $?. Well, the eigenvector matrix of covarience matrix is $ R $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf8c7f",
   "metadata": {},
   "source": [
    "This is the definition of covariance:\n",
    "\n",
    "$$\n",
    "    cov(x, y) = \\frac {\\sum^n_{i=1} (x_i - \\overline{x}) (y_i - \\overline{y})} {n - 1} \\tag{3}\n",
    "$$\n",
    "\n",
    "Covariance shows how two variables change. It's positive if they one increases and the other one increases, or both decreases, and the magnitude shows extent of the second variable responds on first one's change.\n",
    "\n",
    "$$\n",
    "    x \\uparrow \\small\\bigwedge y \\uparrow \\iff cov(x, y) > 0 \\tag{4}\n",
    "$$\n",
    "\n",
    "When both arguments to covariance is same, then it's equal to variance (as our data is normalised, mean is equal to zero).\n",
    "\n",
    "$$\n",
    "    cov(x, x) = \\frac {\\sum^n_{i=1} (x_i - \\overline{x})^2} {n - 1} = \\frac {\\sum^n_{i=1} x_i^2} {n - 1} \\tag{5}\n",
    "$$\n",
    "\n",
    "And covariance matrix is defined as\n",
    "\n",
    "$$\n",
    "    C = \\begin{bmatrix}\n",
    "            cov(x, x) & cov(x, y) \\\\\n",
    "            cov(x, y) & cov(y, y)\n",
    "        \\end{bmatrix} \\tag{6}\n",
    "$$\n",
    "\n",
    "For white data, variance of both variables is $ 1 $, and $ cov(x, y) = 0 $ because x and y is independent.\n",
    "\n",
    "$$\n",
    "    C_D = \\begin{bmatrix}\n",
    "            1 & 0 \\\\\n",
    "            0 & 1\n",
    "        \\end{bmatrix} = I \\tag{7}\n",
    "$$\n",
    "\n",
    "Substitute definition of covariance into covariance matrix:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        C &= \\begin{bmatrix}\n",
    "                \\frac {\\sum^n_{i=1} x_i^2} {n - 1} & \\frac {\\sum^n_{i=1} x_i y_i} {n - 1} \\\\\n",
    "                \\frac {\\sum^n_{i=1} x_i y_i} {n - 1} & \\frac {\\sum^n_{i=1} y_i^2} {n - 1}\n",
    "            \\end{bmatrix} \\\\\n",
    "        &= \\frac {1}{n - 1}\n",
    "            \\begin{bmatrix}\n",
    "                \\sum^n_{i=1} x_i^2 & \\sum^n_{i=1} x_i y_i \\\\\n",
    "                \\sum^n_{i=1} x_i y_i & \\sum^n_{i=1} y_i^2\n",
    "            \\end{bmatrix} \\\\\n",
    "        &= \\frac {1}{n - 1}\n",
    "            \\begin{bmatrix}\n",
    "                x_1 & x_2 & \\cdots & x_n \\\\\n",
    "                y_1 & y_2 & \\cdots & y_n\n",
    "            \\end{bmatrix}\n",
    "            \\begin{bmatrix}\n",
    "                x_1 & y_1 \\\\\n",
    "                x_2 & y_2 \\\\\n",
    "                \\vdots & \\vdots \\\\\n",
    "                x_n & y_n\n",
    "            \\end{bmatrix} \\\\\n",
    "        &= \\frac {1}{n - 1} D D^T \\tag{8}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Substitute definition of $ D' $:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        D' &= RSD \\tag{1} \\\\\n",
    "        C_{D'} &= \\frac {1}{n - 1} D' D'^T \\tag{8} \\\\\n",
    "            &= \\frac {1}{n - 1} RSD (RSD)^T \\\\\n",
    "            &= \\frac {1}{n - 1} RSDD^{\\rm T}S^{\\rm T}R^{\\rm T} \\\\\n",
    "            &= RS \\left( \\frac {1}{n - 1} DD^{\\rm T} \\right) S^{\\rm T}R^{\\rm T} \\\\\n",
    "            &= RS C_D S^{\\rm T}R^{\\rm T} \\\\\n",
    "            &= RSS^{\\rm T}R^{\\rm T} \\\\\n",
    "            &= RSSR^{-1} & S \\text{ is symmetric, } R^T = R^{-1} \\\\\n",
    "            &= RLR^{-1} & \\text{let } L = S^2\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e02135",
   "metadata": {},
   "source": [
    "Doesn't $ RLR^{-1} $ look like diagonalisation?\n",
    "\n",
    "A pair of eigenvalue $ λ $ and eigenvector $ v $ satisfy\n",
    "\n",
    "$$\n",
    "    Cv = λv \\tag{9}\n",
    "$$\n",
    "\n",
    "And we stack up all pairs of eigenvalues $ λ_1 $, $ \\ldots $, $ λ_k $ and eigenvectors $ v_1 $, $ \\ldots $, $ v_k $ ($ k $ is dimensionality):\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        C \\begin{bmatrix} v_1 & \\cdots & v_k \\end{bmatrix} &=\n",
    "            \\begin{bmatrix} v_1 & \\cdots & v_k \\end{bmatrix}\n",
    "            \\begin{bmatrix}\n",
    "                λ_1 & \\cdots & 0 \\\\\n",
    "                \\vdots & \\ddots & 0 \\\\\n",
    "                0 & \\cdots & λ_k\n",
    "            \\end{bmatrix} \\\\\n",
    "        CR &= RL \\\\\n",
    "        C &= RLR^{-1}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, $ R $ is the eigenvector matrix of covariance matrix, but what's the meaning of $ L $?\n",
    "\n",
    "As we know $ R $, we can transform our data into which $ x $ and $ y $ are independent (before rotation, actually $ R^{-1}D' $ or $ SD $). What's the covariance matrix of $ SD $, yes it's $ L $, as $ x $ and $ y $ are stretch by factors $ a $ and $ b $, so variance along $ x $ is $ a^2 $ and along $ y $ is $ b^2 $, and $ cov(x, y) = 0 $.\n",
    "\n",
    "Steps to do PCA:\n",
    "1. Normalise dataset\n",
    "2. Find covariance matrix\n",
    "3. Calculate eigenvector matrix $ R $ and eigenvalue matrix $ L $\n",
    "4. Transform dataset by applying $ R^{-1} $ and then $ L^\\frac 12 $"
   ]
  },
  {
   "cell_type": "code",
   "id": "6d2b71c18da2b844",
   "metadata": {},
   "source": [
    "# add code of PCA, use it on a dataset and show performance improvement\n",
    "# in practice, we normally maintain 90% to 95% variance\n",
    "\n",
    "# n_components=0.90 or n_components=10\n",
    "# define the variance or number of components remain\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "breast_cancer = load_breast_cancer(as_frame=True)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_reduced = pca.fit_transform(breast_cancer.data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2477e5ff",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.1)\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c52e15e6",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, breast_cancer.target, test_size=0.1)\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af8e1d34b63257bf",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "_How you ever thought about how does your brain works? How can pulses transmitted in your neural network leads your decisions?_\n",
    "\n",
    "A neural network is a computational model inspired by biological neural systems, consisting of interconnected layers of artificial neurons (nodes). These layers typically include an input layer to receive data, hidden layers to process information through weighted connections, and an output layer to produce results.\n",
    "\n",
    "By applying activation functions (e.g., ReLU, sigmoid) to introduce non-linearity, neural networks can learn complex patterns through training. They adjust connection weights using optimization algorithms like gradient descent and backpropagation to minimize prediction errors.\n",
    "\n",
    "Neural networks are widely used in machine learning, neural networks power applications ranging from image recognition and natural language processing to decision-making systems. Advanced variants include convolutional neural networks (CNNs) for visual data and recurrent neural networks (RNNs) for sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95cf9a1",
   "metadata": {},
   "source": [
    "## 1. Gradient descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize loss functions in machine learning. It iteratively adjusts model parameters by moving in the direction of steepest descent (negative gradient) of the loss surface. The learning rate (α) determines step sizes. Variants include:\n",
    "\n",
    "- Batch: Uses full dataset per iteration\n",
    "- Stochastic: Uses single sample per iteration\n",
    "- Mini-batch: Uses small data subsets"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1f1a8b5",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Add bias term (x0=1)\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Gradient Descent parameters\n",
    "eta = 0.1  # Learning rate\n",
    "n_iterations = 100\n",
    "m = 100  # Number of samples\n",
    "\n",
    "# Initialize random weights\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# Store loss history\n",
    "loss_history = []\n",
    "\n",
    "# Perform gradient descent\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)  # Compute gradient\n",
    "    theta = theta - eta * gradients  # Update weights\n",
    "    loss = (1 / m) * np.sum((X_b.dot(theta) - y) ** 2)  # Calculate MSE\n",
    "    loss_history.append(loss)\n",
    "\n",
    "# Results\n",
    "print(f\"Optimal parameters:\\n{theta}\")\n",
    "\n",
    "# Plot convergence\n",
    "plt.plot(range(n_iterations), loss_history)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Gradient Descent Convergence\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d15d9686",
   "metadata": {},
   "source": [
    "## 2. Multilayer Perceptron (MLP)\n",
    "\n",
    "An MLP is a class of feedforward artificial neural networks with at least one hidden layer. Unlike simple perceptrons, MLPs can learn non-linear decision boundaries using activation functions (e.g., ReLU, sigmoid). Key features:\n",
    "\n",
    "- Layers: Input → Hidden (≥1) → Output\n",
    "- Backpropagation: Updates weights via chain rule\n",
    "- Universal approximator: Can model complex functions\n",
    "- Used for classification/regression tasks in domains like computer vision and tabular data analysis.\n",
    "\n",
    "**Universal Approximation Theorem**: A 1-hidden-layer MLP with sufficient neurons can approximate any continuous function\n",
    "\n",
    "The MLP consists of three or more layers (an input and an output layer with one or more hidden layers) of nonlinearly-activating nodes. Since MLPs are fully connected, each node in one layer connects with a certain weight.\n",
    "\n",
    "![MLP](img/MLP.png)\n",
    "\n",
    "Learning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation, a generalization of the least mean squares algorithm in the linear perceptron.\n",
    "\n",
    "Consider a simple MLP with one hidden layer of linear transformation $ y = wx + b $ and output layer just output values from that hidden layer. The data point is $ x = 1.5 $, $ y_{gt} = 0.8 $, the initial hyperparameters of hidden layers are $ w = [ 0.8 ] $ and $ b = 0.2 $.\n",
    "\n",
    "![backpropagation](img/backpropagation.png)\n",
    "\n",
    "Let our loss function $ L $ be squared error with coefficient 0.5, the initial loss is 0.18.\n",
    "\n",
    "To optimise MLP we can adjust hyperparameters $ w $ and $ b $ in the reverse direction of partial derivative ($ ε $ is the learning rate, to ensure gradient descent convergent).\n",
    "\n",
    "We can calculate partial derivative of loss function $ L $ over predicted value $ \\hat{y} $ to simplify calculation of partial derivatives of $ L $ over each parameter, using chain rule.\n",
    "\n",
    "In this step, we already witnessed backpropagation from output layer to hidden layer! It's basically about chain rule.\n",
    "\n",
    "Then we calculate new values of hyperparameters and loss function (let learning rate $ ε $ to be 0.1):\n",
    "\n",
    "Loss decreases from 0.18 to 0.082.\n",
    "\n",
    "When we have more hidden layers, optimisation of each layer's parameters requires partial derivatives of loss function over next layer (closer to output layer).\n",
    "\n",
    "![backpropagation](img/backpropagation2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6150968a6df80bd",
   "metadata": {},
   "source": [
    "## 3. Pytorch\n",
    "\n",
    "PyTorch is an open-source machine learning library for Python, widely used for deep learning applications. Developed by Facebook's AI Research team, it offers dynamic computation graphs (define-by-run), allowing flexible model building and debugging. Key features include automatic differentiation (autograd) for gradient calculation, extensive pre-trained models via libraries like TorchVision and TorchText, and strong community support. PyTorch is popular in both research and industry for tasks like natural language processing, computer vision, and reinforcement learning.\n",
    "\n",
    "Let's import pytorch packages and try our first example. It's about fitting a cubic function with a MLP of single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "id": "f1f5d6035147e29a",
   "metadata": {},
   "source": [
    "# build up a neural network by hand, use the one hidden layer nn shown in the prev section\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# target function\n",
    "f = lambda x: 2 * x ** 3 - 3 * x ** 2 + 4 * x - 1\n",
    "\n",
    "\n",
    "# define neural network\n",
    "class CubicNet(nn.Module):\n",
    "    def __init__(self, hidden_size=10):\n",
    "        super(CubicNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, hidden_size)  # input -> hidden\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # hidden -> output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# init MLP model\n",
    "model = CubicNet(hidden_size=20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "faab9a28ac5a0896",
   "metadata": {},
   "source": [
    "# generate data\n",
    "x = torch.linspace(-10, 10, 200).unsqueeze(1)  # shape: (num_samples, 1)\n",
    "y = f(x)\n",
    "y += 0.1 * torch.randn_like(y)\n",
    "\n",
    "# define loss function and optmiser\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# train\n",
    "num_epochs = 6000\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    # forward propagation\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    # backward propagation and optimise\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # record learning curve\n",
    "    loss_history.append(loss.item())\n",
    "    if (epoch + 1) % 300 == 0: print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# visualisation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x.numpy(), y.numpy(), label='True Data', alpha=0.5)\n",
    "plt.plot(x.numpy(), predictions.numpy(), 'r', lw=2, label='Predictions')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function Fitting')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "897f6551d0c7a9c1",
   "metadata": {},
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "\n",
    "# load model\n",
    "model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97c75f635fd9c178",
   "metadata": {},
   "source": [
    "# test model on test set 1 \n",
    "# test set 1 contains 100 datapoints in [-10,10]\n",
    "x = torch.linspace(-10, 10, 100).unsqueeze(1)\n",
    "y = f(x)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "    print(torch.mean((y_pred - y) ** 2))  # loss should be low (< 1000)\n",
    "    \n",
    "# plot fit curve on test set\n",
    "plt.scatter(x.numpy(), y.numpy(), label='True Data', alpha=0.5)\n",
    "plt.plot(x.numpy(), y_pred.numpy(), 'r', lw=2, label='Predictions')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function Fitting')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7262b66b9c3d97f5",
   "metadata": {},
   "source": [
    "# test model on test set 2\n",
    "# test set 2 contains 100 datapoints in [10,20]\n",
    "x = torch.linspace(10, 20, 100).unsqueeze(1)\n",
    "y = f(x)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "    print(torch.mean((y_pred - y) ** 2))  # loss should be extremely high (> 10**7)\n",
    "\n",
    "# plot fit curve on test set\n",
    "plt.scatter(x.numpy(), y.numpy(), label='True Data', alpha=0.5)\n",
    "plt.plot(x.numpy(), y_pred.numpy(), 'r', lw=2, label='Predictions')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function Fitting')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d4697bc52c74633",
   "metadata": {},
   "source": [
    "_Think: what causes the differences in accuracy on these 2 test sets?_\n",
    "- nn doesn't work well on entirely strange data\n",
    "- actually all machine learning methods are like this, because they learnt from training set, they can only know what they've learnt\n",
    "- generalization ability (opposite to overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3219d5bdeaaba57",
   "metadata": {},
   "source": [
    "Exercise: can you define a MLP with 10 input neurons, 2 hidden layers, and a single output\n",
    "\n",
    "Use your new defined MLP on the task above, then run the tests, what do you notice? what do you think cause the consequence?"
   ]
  },
  {
   "cell_type": "code",
   "id": "d54a9d86dd5ddb89",
   "metadata": {},
   "source": [
    "# write your code\n",
    "\n",
    "class NewCubicNet(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden1=64, hidden2=32, output_size=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1)  # input -> hidden 1\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)  # hidden 1 -> hidden 2\n",
    "        self.fc3 = nn.Linear(hidden2, output_size)  # hidden 2 -> output\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = NewCubicNet(input_size=10)  # input dimensionality is 10\n",
    "\n",
    "# generate data\n",
    "x = torch.linspace(-10, 10, 100).unsqueeze(1)\n",
    "y = f(x)\n",
    "\n",
    "try:\n",
    "    outputs = model(x)  # this raises error as actual input data is one-dimensional\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "# solution 1: change input_size back to 1\n",
    "# model = NewCubicNet(input_size=1)\n",
    "# solution 2: expand input to 10D (feature engineering)\n",
    "x = torch.cat([x ** i for i in range(10)], dim=1)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "    print(torch.mean((y_pred - y) ** 2))  # ummm, doesn't matter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bfb073f806c90625",
   "metadata": {},
   "source": [
    "## 4. Convolutional Neural Networks (CNN)\n",
    "\n",
    "Convolutional Neural Network (CNN) is a deep learning architecture specialized for processing grid-like data (e.g., images), is widely used in computer vision tasks like image recognition, object detection, and medical imaging. Its core components are:\n",
    "\n",
    "- Convolutional Layers: Extract local features using learnable filters/kernels (e.g., edges, textures).\n",
    "- Activation Functions (e.g., ReLU): Introduce non-linearity to capture complex patterns.\n",
    "- Pooling Layers (e.g., MaxPooling): Reduce spatial dimensions while retaining key features.\n",
    "- Fully Connected Layers: Combine high-level features for classification/regression.\n",
    "\n",
    "Key Advantages:\n",
    "\n",
    "- Hierarchical Feature Learning: Automatically learns low → high-level features (edges → shapes → objects).\n",
    "- Parameter Sharing: Filters scan the entire input, reducing parameters vs. dense layers.\n",
    "- Translation Invariance: Recognizes patterns regardless of position (via pooling).\n",
    "\n",
    "Example Workflow (Image Classification):\n",
    "\n",
    "Input → Conv → ReLU → Pool → Conv → ReLU → Pool → Flatten → FC → Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341bab0",
   "metadata": {},
   "source": [
    "Pooling layers are layers where neurons only has local views, instead of full connections to neurons of previous layers. This reduces number of connections and ease the pressure of overfitting. In max pooling layer, each neuron passes largest value of local views to next layer.\n",
    "\n",
    "![convolutional layer](img/convolutional_layer.gif)\n",
    "\n",
    "For max pooling layers, the backpropagation requires recording the maximum value position of each pooling window at the time of forward propagation, and the gradient is only passed to these locations, and the rest of the positions have a gradient of 0.\n",
    "\n",
    "For example, we pass a matrix of size 2x2 to a max pooling layer with window size 2x2:\n",
    "\n",
    "$$\n",
    "    \\text{input } x \\rightarrow \\text{pooling kernal } w \\rightarrow \\text{output } y\n",
    "$$\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & 2 \\\\\n",
    "        3 & 4\n",
    "    \\end{bmatrix}\n",
    "    \\rightarrow \\text{pooling window 2x2} \\rightarrow\n",
    "    \\begin{bmatrix}\n",
    "        0 & 0 \\\\\n",
    "        0 & 4\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For backpropagation let's assume the gradient of output given by next layer (closer to output layer) is 2:\n",
    "\n",
    "$$\n",
    "    \\text{input } x \\leftarrow \\text{pooling kernal } w \\leftarrow \\text{output } y\n",
    "$$\n",
    "$$\n",
    "    \\frac {∂L}{∂x} =\n",
    "        \\begin{bmatrix}\n",
    "            0 & 0 \\\\\n",
    "            0 & 2\n",
    "        \\end{bmatrix}\n",
    "    \\leftarrow \\text{pooling window 2x2} \\leftarrow \\frac {∂L}{∂y} = 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8c4e1",
   "metadata": {},
   "source": [
    "Convolutional layers are also layers where neurons only has local views. Each neuron does a full convolution with local view, which is multiplication between mapping cells in local view and convolution kernel (not matrix multiplication).\n",
    "\n",
    "After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map.\n",
    "\n",
    "For example, we pass a matrix of size 3x3 to a convolutional layer with window size 2x2:\n",
    "\n",
    "$$\n",
    "    \\text{input } x \\rightarrow \\text{convolutional kernal } w \\rightarrow \\text{output } y\n",
    "$$\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & 1 & 1 \\\\\n",
    "        0 & 0 & 1 \\\\\n",
    "        0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\rightarrow\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 \\\\\n",
    "        0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\rightarrow\n",
    "    \\begin{bmatrix}\n",
    "        1 & 2 \\\\\n",
    "        0 & 2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For backpropagation, the gradient of convolutional kernal is the result of convolution between kernal and output gradient (output gradient is a random picked matrix):\n",
    "\n",
    "$$\n",
    "    \\text{input } x \\leftarrow \\text{convolutional kernal } w \\leftarrow \\text{output } y\n",
    "$$\n",
    "$$\n",
    "    \\frac {∂L}{∂x} = w ⊛ \\frac{∂L}{∂y} =\n",
    "        \\begin{bmatrix}\n",
    "            0 & 2 \\\\\n",
    "            0 & 2\n",
    "        \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\frac {∂L}{∂w} = x ⊛ \\frac{∂L}{∂y} =\n",
    "        \\begin{bmatrix}\n",
    "            3 & 5 \\\\\n",
    "            0 & 4\n",
    "        \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\frac {∂L}{∂y} =\n",
    "        \\begin{bmatrix}\n",
    "            1 & 0 \\\\\n",
    "            0 & 1\n",
    "        \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc557a1f499828a",
   "metadata": {},
   "source": [
    "## 5. MNIST dataset\n",
    "\n",
    "The MNIST dataset (Modified National Institute of Standards and Technology database) is a widely used benchmark dataset in machine learning and computer vision. It consists of 70,000 grayscale images of handwritten digits from 0 to 9, divided into 60,000 training images and 10,000 test images.\n",
    "\n",
    "Each image is 28x28 pixels in size, with pixel values ranging from 0 (black) to 255 (white). The dataset includes corresponding labels indicating the digit (0–9) represented in each image."
   ]
  },
  {
   "cell_type": "code",
   "id": "fdb7cbee",
   "metadata": {},
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# preprocessing\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0,), (1,))\n",
    "])\n",
    "\n",
    "# load dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# create data loader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f903c38bbf6a2cd",
   "metadata": {},
   "source": [
    "# define neural network\n",
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))  # [B, 32, 28, 28]\n",
    "        x = self.pool(x)  # [B, 32, 14, 14]\n",
    "        x = self.relu(self.conv2(x))  # [B, 64, 14, 14]\n",
    "        x = self.pool(x)  # [B, 64, 7, 7]\n",
    "        x = x.view(-1, 64 * 7 * 7)  # flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# initialise model and running device\n",
    "model = DigitClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bbab56f0023997d5",
   "metadata": {},
   "source": [
    "# define loss function and optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 10\n",
    "train_losses, test_accuracies = [], []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # forward propagation\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # record loss\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # average loss\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # test accuracy\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # average accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, \"\n",
    "          f\"Test Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b7858d34db6f47e",
   "metadata": {},
   "source": [
    "# visualisation\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.plot(test_accuracies)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.show()\n",
    "\n",
    "# example prediction\n",
    "with torch.no_grad():\n",
    "    img = test_dataset.data[0]\n",
    "    img_data = img.unsqueeze(0).float()\n",
    "    prediction = torch.argmax(model(img_data), dim=1).item()\n",
    "    plt.imshow(img, cmap=\"grey\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Predicted Label: {prediction}\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6912ba1150dc3a1d",
   "metadata": {},
   "source": [
    "# Wrap up and final words\n",
    "\n",
    "- PCA: High-dimensional data leads to an increase in data sparsity, an exponential growth in computational complexity, and a decrease in the efficiency of traditional analytical methods.\n",
    "- NN: Imitating the structure of human brain neurons, learning complex patterns through multiple layers of nonlinear transformations.\n",
    "- MLP: The most basic neural network structure, consisting of an input layer, a hidden layer, and an output layer.\n",
    "- CNN: Simulates the biological visual system and extracts local features through convolutional kernels (filters), with the same convolutional kernel slides across the whole image, reducing the number of parameters and improving computing efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
